# -*- coding: utf-8 -*-
"""topic_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YggGUjZNij3IpH7tOFYQXC5J9zwiWz2s

## 1. News Crawling
네이버 뉴스>경제>금융

크롤링 시점 하루 전 모든 금융 기사 수집

(예외)월요일에 크롤링 : 금~일 기사
"""

#크롤링시 필요한 라이브러리 불러오기
from bs4 import BeautifulSoup
import requests
import re
from datetime import datetime, timedelta
from tqdm import tqdm
import sys
import pandas as pd

#ConnectionError방지
headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102"}

#날짜 넣으면 마지막 페이지 알려주는 함수
def lastPage(date):
  formated_date = date.strftime('%Y%m%d')
  page_url = "https://news.naver.com/main/list.naver?mode=LS2D&sid2=259&sid1=101&mid=shm&date=" + formated_date+ "&page=50"
  original_html = requests.get(page_url,headers=headers)
  html = BeautifulSoup(original_html.text, "html.parser")
  last_page = html.select_one("#main_content > div.paging > strong")
  last_num = int(last_page.get_text())

  return last_num

# 네이버 뉴스 페이지 url 생성하는 함수
def makePageUrl():
    urls = []
    today = datetime.now()
    # today = datetime.now() - timedelta(days=1)
    today_ = today.weekday()

    #크롤링 시점이 월요일일 때
    if today_ == 0:
      for ds in range(1,4):
        date = today - timedelta(days=ds)
        formated_date = date.strftime('%Y%m%d')
        last_page = lastPage(date)

        for i in range(1,last_page + 1):
          pg = str(i)
          url = "https://news.naver.com/main/list.naver?mode=LS2D&sid2=259&sid1=101&mid=shm&date=" + formated_date+ "&page=" + pg
          urls.append(url)

    #크롤링 시점이 화~금요일 일 때
    else:
      date = datetime.now() - timedelta(days=1)
      formated_date = date.strftime('%Y%m%d')
      last_page = lastPage(date)

      for i in range(1,last_page + 1):
        pg = str(i)
        url = "https://news.naver.com/main/list.naver?mode=LS2D&sid2=259&sid1=101&mid=shm&date=" + formated_date+ "&page=" + pg
        urls.append(url)

    return urls

#네이버 뉴스 페이지에 있는 기사 url수집 함수
#head와 head가 아닌 태그로 이루어져있어서 두가지 함수로 활용
#head 10개 먼저 채워져야 head가 아닌 기사 존재

def articles_crawler_head(page_url,n): #n은 1~10
    original_html = requests.get(page_url,headers=headers)
    html = BeautifulSoup(original_html.text, "html.parser")
    url_news = html.select_one("#main_content > div.list_body.newsflash_body > ul.type06_headline > li:nth-child("+ str(n) +") > dl > dt:nth-child(2) > a")
    url = url_news.attrs['href']
    return url

def articles_crawler(page_url,n): #n은 1~10
    original_html = requests.get(page_url,headers=headers)
    html = BeautifulSoup(original_html.text, "html.parser")

    url_news = html.select_one("#main_content > div.list_body.newsflash_body > ul.type06 > li:nth-child("+ str(n) +") > dl > dt > a")
    url = url_news.attrs['href']
    return url

#네이버 뉴스 페이지 url 생성
page_urls = makePageUrl()

#해당 페이지에 있는 기사 url 수집
news_url =[]

for i in page_urls:
    for m in range(1,11):
        try:
            url = articles_crawler_head(i,m)
            news_url.append(url)
        except AttributeError:
            continue

        try:
            url = articles_crawler(i,m)
            news_url.append(url)
        except AttributeError:
            pass

#혹시모를 중복url제거
news_urls = list(set(news_url))

#뉴스 제목, 언론사, 뉴스가 작성된 날짜, 기사 본문 수집
news_titles = []
news_press = []
news_dates = []
news_contents =[]

for i in tqdm(news_urls):
    #각 기사 html get하기
    news = requests.get(i)
    news_html = BeautifulSoup(news.text,"html.parser")

    # 뉴스 제목 가져오기
    title = news_html.select_one("#ct > div.media_end_head.go_trans > div.media_end_head_title > h2")
    if title == None:
        title = news_html.select_one("#content > div.end_ct > div > h2")

    # 언론사 가져오기
    try:
      press = news_html.select_one(".media_end_head_top_logo > img")['title']
    except TypeError:
      press = news_html.select_one("#_CHANNEL_LAYER_366_0000868613 > span.media_end_head_top_channel_layer_text > strong")

    #날짜 가져오기
    try:
        html_date = news_html.select_one("._ARTICLE_DATE_TIME")
        news_date = html_date.get_text('data-date-time')
    except AttributeError:
        news_date = news_html.select_one("#content > div.end_ct > div > div.article_info > span > em")
        news_date = re.sub(pattern=pattern1,repl='',string=str(news_date))

    # 뉴스 본문 가져오기
    content = news_html.select("#dic_area")
    if content == []:
        content = news_html.select("#articeBody")

    # 기사 텍스트만 가져오기
    # list합치기
    content = ''.join(str(content))

    # html태그제거 및 텍스트 다듬기
    pattern1 = '<[^>]*>'
    title = re.sub(pattern=pattern1, repl='', string=str(title))
    content = re.sub(pattern=pattern1, repl='', string=content)
    press = re.sub(pattern=pattern1, repl='', string=str(press))
    pattern2 = """[\n\n\n\n\n// flash 오류를 우회하기 위한 함수 추가\nfunction _flash_removeCallback() {}"""
    content = content.replace(pattern2, '')

    news_titles.append(title)
    news_press.append(press)
    news_dates.append(news_date)
    news_contents.append(content)

#데이터 프레임 만들기
news_df = pd.DataFrame({'title':news_titles,'link':news_urls,'press':news_press,'datetime':news_dates,'content':news_contents})

#중복 행 지우기
news_df = news_df.drop_duplicates(keep='first',ignore_index=True)
print("중복 제거 후 행 개수: ",len(news_df))

# #데이터 프레임 저장
# today = datetime.now() - timedelta(days=1)
# news_df.to_csv('{}_{}.csv'.format('today_news',today.strftime('%Y%m%d')),encoding='utf-8-sig',index=False)

"""## 2. BERTopic

### 2-0) BERTopic install
"""

"""### 2-1) 전처리"""

from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer
from konlpy.tag import Mecab
from bertopic import BERTopic

data = news_df

rm = ['\r','\n','\t',r'\u','“','”','‘','▲','■',"'",'"','\xa0',\
      '(서울=연합뉴스)','[연합뉴스 자료사진]','(서울=뉴스1)','[서울=뉴시스]','게티이미지뱅크',
      '기자','제공','금융','뉴스','사진','스포츠서울','연합뉴스','기사문의 및 제보','앵커']

for r in rm:
  data['content'] = data['content'].apply(lambda x : x.replace(r,''))

data = data[data['content'].apply(lambda x: x.strip() != '')]

from datetime import datetime

today = datetime.now()
today = today.strftime('%Y%m%d')

text_data = './raw_data/'+ today +'.txt'

with open(text_data, 'w') as f:
    for value in data['content']:
        f.write(f"{value}\n")

documents = [line.strip() for line in open(text_data, encoding="utf-8").readlines()]

preprocessed_documents = []

for line in tqdm(documents):
  # 빈 문자열이거나 숫자로만 이루어진 줄은 제외
  if line and not line.replace(' ', '').isdecimal():
    preprocessed_documents.append(line)

"""### 2-2) Mecab과 SBERT를 이용한 Bertopic"""

class CustomTokenizer:
    def __init__(self, tagger):
        self.tagger = tagger
    def __call__(self, sent):
        sent = sent[:1000000]
        word_tokens = self.tagger.nouns(sent)
        # word_tokens = self.tagger.morphs(sent)
        result = [word for word in word_tokens if len(word) > 1]
        return result

custom_tokenizer = CustomTokenizer(Mecab())

vectorizer = CountVectorizer(tokenizer=custom_tokenizer, max_features=8000)

model = BERTopic(embedding_model="sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens", \
                 vectorizer_model=vectorizer,
                 top_n_words=10,
                 calculate_probabilities=True)

topics, probs = model.fit_transform(preprocessed_documents)

model.visualize_topics()

model.visualize_distribution(probs[0])

news_df['topic_num'] = topics

words_list = []
for i in range(0, 50):
  gettopic = model.get_topic(i)
  try:
    words = [j[0] for j in gettopic]
    words_list.append(words)
    print(i,'번째 토픽 :', words)
  except TypeError:
    pass

topic_words_df = pd.DataFrame({'words':words_list})

TP_contents = {'topic_num' : topics, 'cnts' : preprocessed_documents}
df = pd.DataFrame(TP_contents)
topic_cnts = df.groupby('topic_num')['cnts'].sum()
df.groupby('topic_num')['cnts'].count()

"""## 3. TextRank : 주요 문장 추출"""


def textRank(docs):
  result_sent = []
  from textrank import KeysentenceSummarizer

  summarizer = KeysentenceSummarizer(
      tokenize = lambda x:x.split(),
      min_sim = 0.3,
      verbose = False
  )
  keysents = summarizer.summarize(sents, topk=3)
  for _, _, sent in keysents:
      result_sent.append(sent)

  return result_sent

summerize = []
for cts in topic_cnts:
  sm = textRank(cts)
  summerize.append(sm)

topic_words_df['summerize'] = summerize[1:]

"""## 4. json으로 export"""

import os
import datetime

def create_date_directory(base_path):
    # 현재 날짜 정보 가져오기
    current_date = datetime.date.today()

    # 년, 월, 일 추출
    year = current_date.year
    month = current_date.strftime('%m')
    day = current_date.strftime('%d')

    # 년 폴더 경로 생성
    year_dir = os.path.join(base_path, str(year))

    # 년 폴더가 없으면 생성
    if not os.path.exists(year_dir):
        os.makedirs(year_dir)

    # 월 폴더 경로 생성
    month_dir = os.path.join(year_dir, month)

    # 월 폴더가 없으면 생성
    if not os.path.exists(month_dir):
        os.makedirs(month_dir)

    # 일 폴더 경로 생성
    day_dir = os.path.join(month_dir, day)

    # 일 폴더 생성 (이미 존재하면 에러를 발생시키지 않음)
    os.makedirs(day_dir, exist_ok=True)

    return day_dir

base_directory = 'json_data/'  # 기본 디렉토리 경로를 설정하세요.
create_date_directory(base_directory)

import json

today = datetime.now()
today = today.strftime('%Y%m%d')

# 저장할 경로와 파일 이름 설정
news_file_path = base_directory + today.strftime('%Y/%m/%d') + "/news.json"
topics_file_path = base_directory + today.strftime('%Y/%m/%d') + "/topics.json"

# JSON 데이터를 파일에 쓰기
news_df.to_json(news_file_path,force_ascii=False,indent=4)
topic_words_df.to_json(topics_file_path,force_ascii=False,indent=4)